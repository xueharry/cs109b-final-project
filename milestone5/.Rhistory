imdb5k <- read.csv('imdb5konehot.csv')
nums <- sapply(imdb5k, is.numeric)
imdb5k <- imdb5k[, nums]
for(i in 1:ncol(imdb5k)){
imdb5k[is.na(imdb5k[,i]), i] <- mean(imdb5k[,i], na.rm = TRUE)
}
imdb5k.mldr <- mldr_from_dataframe(dataframe=imdb5k, labelIndices=which(startsWith(colnames(imdb5k.clean), "i_")))
ds <- create_holdout_partition(imdb5k.mldr, c(train=0.80, test=0.20))
br.model <- br(ds$train, "RF", seed=123)
library("caTools")
library("utiml")
imdb5k <- read.csv('imdb5konehot.csv')
nums <- sapply(imdb5k, is.numeric)
imdb5k <- imdb5k[, nums]
library("caTools")
library("utiml")
imdb5k <- read.csv('imdb5konehot.csv')
imdb5k <- read.csv('imdb5konehot.csv')
imdb5k <- imdb5k[, sapply(imdb5k, is.numeric)]
for(i in 1:ncol(imdb5k)){
imdb5k[is.na(imdb5k[,i]), i] <- mean(imdb5k[,i], na.rm = TRUE)
}
imdb5k.mldr <- mldr_from_dataframe(dataframe=imdb5k, labelIndices=which(startsWith(colnames(imdb5k.clean), "i_")))
imdb5k.mldr <- mldr_from_dataframe(dataframe=imdb5k, labelIndices=which(startsWith(colnames(imdb5k), "i_")))
ds <- create_holdout_partition(imdb5k.mldr, c(train=0.80, test=0.20))
br.model <- br(ds$train, "RF", seed=123)
prediction.br <- predict(br.model, ds$test)
head(as.bipartition(prediction.br))
br.result <- multilabel_evaluate(ds$tes, prediction.br, "bipartition")
rakel.model <- rakel(ds$train, "RF", cores=2)
prediction.rakel <- predict(rakel.model, ds$test)
head(as.bipartition(prediction.rakel))
rakel.result <- multilabel_evaluate(ds$tes, prediction.rakel, "bipartition")
result <- data.frame(
br = br.result,
rakel = rakel.result,
# eps = eps.result,
# homer = homer.result
)
result <- data.frame(
br = br.result,
rakel = rakel.result,
# eps = eps.result,
# homer = homer.result
)
result <- data.frame(
br = br.result,
rakel = rakel.result
# eps = eps.result,
# homer = homer.result
)
plot(result)
br.result
result <- cbind(
br = br.result,
rakel = rakel.result
# eps = eps.result,
# homer = homer.result
)
plot(result)
print(round(result, 3))
type(result)
class(result)
library("corrplot")
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
library("corrplot")
imdb5k <- read.csv('imdb5konehot.csv')
imdb5k <- read.csv('imdb5konehot.csv')
imdb5k <- imdb5k[, sapply(imdb5k, is.numeric)]
imdb5k <- imdb5k[, sapply(imdb5k, is.numeric)]
for(i in 1:ncol(imdb5k)){
imdb5k[is.na(imdb5k[,i]), i] <- mean(imdb5k[,i], na.rm = TRUE)
}
imdb5k.mldr <- mldr_from_dataframe(dataframe=imdb5k, labelIndices=which(startsWith(colnames(imdb5k), "i_")))
ds <- create_holdout_partition(imdb5k.mldr, c(train=0.80, test=0.20))
br.model <- br(ds$train, "RF", cores=2)
prediction.br <- predict(br.model, ds$test)
prediction.br <- predict(br.model, ds$test)
head(as.bipartition(prediction.br))
head(as.bipartition(prediction.br))
br.result <- multilabel_evaluate(ds$tes, prediction.br, "bipartition")
prediction.br <- predict(br.model, ds$test)
prediction.br <- predict(br.model, ds$test)
br.result <- multilabel_evaluate(ds$tes, prediction.br, "bipartition")
ecc.model <- ecc(ds$train, "RF", cores=2)
ecc.model <- ecc(ds$train, "RF", cores=2)
prediction.ecc <- predict(ecc.model, ds$test)
ecc.result <- multilabel_evaluate(ds$tes, prediction.ecc, "bipartition")
prediction.ecc <- predict(ecc.model, ds$test)
prediction.ecc <- predict(ecc.model, ds$test)
ecc.result <- multilabel_evaluate(ds$tes, prediction.ecc, "bipartition")
result <- cbind(
br = br.result,
ecc = ecc.result
# rakel = rakel.result,
# eps = eps.result,
# homer = homer.result
)
print(round(result, 3))
corrplot(result, method="circle")
```{r, fig.height=10, fig.width=3}
corrplot(result, method="circle")
corrplot(result, method="circle")
```{r, fig.height=15, fig.width=15}
corrplot(result, method="circle")
corrplot(result, method="circle")
corrplot(result, method="circle")
corrplot(result, method="number")
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
library("utiml")
library("corrplot")
multilabel_measures()
multilabel_measures()
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
library("utiml")
library("corrplot")
train <- read.csv('full_data/full_data_train.csv')
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
train <- read.csv('full_data/full_data_train.csv')
test <- read.csv('full_data/full_data_test.csv')
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
first.y <- which(colnames(train) == "Mystery")
train.mldr <- mldr_from_dataframe(dataframe=train, labelIndices=first.y:length(test))
test.mldr <- mldr_from_dataframe(dataframe=test, labelIndices=first.y:length(test))
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
library("utiml")
library("corrplot")
train <- read.csv('full_data/full_data_train.csv')
test <- read.csv('full_data/full_data_test.csv')
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
first.y <- which(colnames(train) == "Mystery")
train.mldr <- mldr_from_dataframe(dataframe=train, labelIndices=first.y:length(test))
test.mldr <- mldr_from_dataframe(dataframe=test, labelIndices=first.y:length(test))
## RAKEL ##
print("Started")
print(Sys.time())
# RAKEL with random forests
rakel.model <- rakel(train.mldr, "RF", cores=2)
print("Ended")
print(Sys.time())
# predict
prediction.rakel <- predict(rakel.model, test.mldr)
# Evaluate the models
rakel.result <- multilabel_evaluate(test.mldr, prediction.rakel, "bipartition")
## BINARY RELEVANCE ##
# binary relevance with random forests
br.model <- br(train.mldr, "RF", cores=2)
# predict
prediction.br <- predict(br.model, test.mldr)
# evaluate binary relevance
br.result <- multilabel_evaluate(test.mldr, prediction.br, "bipartition")
## ENSEMBLE OF CLASSIFIER CHAINS ##
# ecc with random forests
ecc.model <- ecc(train.mldr, "RF", cores=2)
# predict
prediction.ecc <- predict(ecc.model, test.mldr)
# evaluate binary relevance
ecc.result <- multilabel_evaluate(test.mldr, prediction.ecc, "bipartition")
## EPS ##
# EPS with random forests
eps.model <- eps(train.mldr, "RF", cores=2)
# predict
prediction.eps <- predict(eps.model, test.mldr)
# Evaluate the models
eps.result <- multilabel_evaluate(test.mldr, prediction.eps, "bipartition")
## HOMER ##
# HOMER with random forests
homer.model <- homer(train.mldr, "RF", cores=2)
# predict
prediction.homer <- predict(homer.model, test.mldr)
# Evaluate the models
homer.result <- multilabel_evaluate(test.mldr, prediction.homer, "bipartition")
# print results
result <- cbind(
br = br.result,
ecc = ecc.result,
rakel = rakel.result,
eps = eps.result,
homer = homer.result
)
print(round(result, 3))
corrplot(result, method="color")
View(test)
train <- train[, -c(1,3)]
test <- test[, -c(1,3)]
library("caret")
?train
ecc.model <- ecc(train.mldr, "RF", cores=2)
prediction.ecc <- predict(ecc.model, test.mldr)
?predict
ecc.result <- multilabel_evaluate(test.mldr, prediction.ecc, "bipartition")
View(test)
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
write.csv(train, 'full_data/clean_train.csv')
write.csv(test, 'full_data/clean_test.csv')
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
library("corrplot")
## PREPARE DATA ##
# load data
imdb5k <- read.csv('imdb5konehot.csv')
# drop categoricals
imdb5k <- imdb5k[, sapply(imdb5k, is.numeric)]
# impute all columns by column mean
for(i in 1:ncol(imdb5k)){
imdb5k[is.na(imdb5k[,i]), i] <- mean(imdb5k[,i], na.rm = TRUE)
}
# create MLDR object
imdb5k.mldr <- mldr_from_dataframe(dataframe=imdb5k, labelIndices=which(startsWith(colnames(imdb5k), "i_")))
# train-test split
ds <- create_holdout_partition(imdb5k.mldr, c(train=0.80, test=0.20))
## BINARY RELEVANCE ##
# binary relevance with random forests
br.model <- br(ds$train, "RF", cores=2)
# predict
prediction.br <- predict(br.model, ds$test, cores=2)
# evaluate binary relevance
br.result <- multilabel_evaluate(ds$tes, prediction.br, "bipartition")
## ENSEMBLE OF CLASSIFIER CHAINS ##
# ecc with random forests
ecc.model <- ecc(ds$train, "RF", cores=2)
# predict
prediction.ecc <- predict(ecc.model, ds$test, cores=2)
# evaluate binary relevance
ecc.result <- multilabel_evaluate(ds$tes, prediction.ecc, "bipartition")
## RAKEL ##
# RAKEL with random forests
rakel.model <- rakel(ds$train, "RF", cores=2)
# predict
prediction.rakel <- predict(rakel.model, ds$test, cores=2)
# Evaluate the models
rakel.result <- multilabel_evaluate(ds$tes, prediction.rakel, "bipartition")
## EPS ##
# EPS with random forests
eps.model <- eps(ds$train, "RF", cores=2)
# predict
prediction.eps <- predict(eps.model, ds$test, cores=2)
# Evaluate the models
eps.result <- multilabel_evaluate(ds$tes, prediction.eps, "bipartition")
## HOMER ##
# HOMER with random forests
homer.model <- homer(ds$train, "RF", cores=2)
# predict
prediction.homer <- predict(homer.model, ds$test, cores=2)
# Evaluate the models
homer.result <- multilabel_evaluate(ds$tes, prediction.homer, "bipartition")
# print results
result <- cbind(
br = br.result,
ecc = ecc.result,
rakel = rakel.result,
eps = eps.result,
homer = homer.result
)
print(round(result, 3))
corrplot(result, method="color")
## PREPARE DATA ##
# load data
train <- read.csv('full_data/full_data_train.csv')
test <- read.csv('full_data/full_data_test.csv')
# make zero budget and zero revenue NA
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
# impute all columns by column median
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
# drop ID columns
train <- train[, -c(1,3)]
test <- test[, -c(1,3)]
# save for posterity
write.csv(train, 'full_data/clean_train.csv')
write.csv(test, 'full_data/clean_test.csv')
# find first y column
first.y <- which(colnames(train) == "Mystery")
# create MLDR objects
train.mldr <- mldr_from_dataframe(dataframe=train, labelIndices=first.y:length(test))
test.mldr <- mldr_from_dataframe(dataframe=test, labelIndices=first.y:length(test))
## BINARY RELEVANCE ##
# binary relevance with random forests
br.model <- br(train.mldr, "RF", cores=2)
# predict
prediction.br <- predict(br.model, test.mldr, cores=2)
# evaluate binary relevance
br.result <- multilabel_evaluate(test.mldr, prediction.br, "bipartition")
## ENSEMBLE OF CLASSIFIER CHAINS ##
# ecc with random forests
ecc.model <- ecc(train.mldr, "RF", cores=2)
# predict
prediction.ecc <- predict(ecc.model, test.mldr, cores=2)
# evaluate binary relevance
ecc.result <- multilabel_evaluate(test.mldr, prediction.ecc, "bipartition")
## RAKEL ##
# RAKEL with random forests
rakel.model <- rakel(train.mldr, "RF", cores=2)
# predict
prediction.rakel <- predict(rakel.model, test.mldr, cores=2)
# Evaluate the models
rakel.result <- multilabel_evaluate(test.mldr, prediction.rakel, "bipartition")
## EPS ##
# EPS with random forests
eps.model <- eps(train.mldr, "RF", cores=2)
# predict
prediction.eps <- predict(eps.model, test.mldr, cores=2)
# Evaluate the models
eps.result <- multilabel_evaluate(test.mldr, prediction.eps, "bipartition")
## HOMER ##
# HOMER with random forests
homer.model <- homer(train.mldr, "RF", cores=2)
# predict
prediction.homer <- predict(homer.model, test.mldr, cores=2)
# Evaluate the models
homer.result <- multilabel_evaluate(test.mldr, prediction.homer, "bipartition")
# print results
result <- cbind(
br = br.result,
ecc = ecc.result,
rakel = rakel.result,
eps = eps.result,
homer = homer.result
)
print(round(result, 3))
corrplot(result, method="color")
View(face.pca.scores.test)
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
train <- read.csv('full_data/full_data_train.csv')
test <- read.csv('full_data/full_data_test.csv')
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
train <- train[, -c(1,3)]
test <- test[, -c(1,3)]
write.csv(train, 'full_data/clean_train.csv')
write.csv(test, 'full_data/clean_test.csv')
first.y <- which(colnames(train) == "Mystery")
train.mldr <- mldr_from_dataframe(dataframe=train, labelIndices=first.y:length(test))
test.mldr <- mldr_from_dataframe(dataframe=test, labelIndices=first.y:length(test))
# EPS with random forests
eps.model <- eps(train.mldr, "RF", cores=2)
# predict
prediction.eps <- predict(eps.model, test.mldr, cores=2)
write.csv(prediction.eps, 'preds/eps.csv')
View(train)
View(test)
## PREPARE DATA ##
# load data
train <- read.csv('full_data/full_data_train.csv')
test <- read.csv('full_data/full_data_test.csv')
# make zero budget and zero revenue NA
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
# impute all columns by column median
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
# drop ID columns
train <- train[, -c(1,3)]
test <- test[, -c(1,3)]
# save for posterity
write.csv(train, 'full_data/clean_train.csv')
write.csv(test, 'full_data/clean_test.csv')
# find first y column
first.y <- which(colnames(train) == "Mystery")
# create MLDR objects
train.mldr <- mldr_from_dataframe(dataframe=train, labelIndices=first.y:length(test))
test.mldr <- mldr_from_dataframe(dataframe=test, labelIndices=first.y:length(test))
## BINARY RELEVANCE ##
# binary relevance with random forests
br.model <- br(train.mldr, "RF", cores=2)
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
library("utiml")
library("corrplot")
train <- read.csv('full_data/full_data_train.csv')
test <- read.csv('full_data/full_data_test.csv')
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
train <- train[, -c(1,3)]
test <- test[, -c(1,3)]
write.csv(train, 'full_data/clean_train.csv')
write.csv(test, 'full_data/clean_test.csv')
first.y <- which(colnames(train) == "Mystery")
train.mldr <- mldr_from_dataframe(dataframe=train, labelIndices=first.y:length(test))
test.mldr <- mldr_from_dataframe(dataframe=test, labelIndices=first.y:length(test))
# HOMER with random forests
homer.model <- homer(train.mldr, "RF", cores=2)
# predict
prediction.homer <- predict(homer.model, test.mldr, cores=2)
write.csv(prediction.homer, 'preds/homer.csv')
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library("caTools")
library("utiml")
library("corrplot")
train <- read.csv('full_data/full_data_train.csv')
test <- read.csv('full_data/full_data_test.csv')
View(test)
train$budget[train$budget == 0] <- NA
train$revenue[train$revenue == 0] <- NA
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- median(train[,i], na.rm = TRUE)
}
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
train <- train[, -1]
test <- test[, -1]
write.csv(train, 'full_data/clean_train.csv')
write.csv(test, 'full_data/clean_val.csv')
train <- train[, -2]
test <- test[, -2]
test <- read.csv('full_data_test.csv')
test <- read.csv('full_data_test.csv')
test <- read.csv('full_data_test.csv')
test <- read.csv('full_data_test.csv')
setwd("~/Downloads")
test <- read.csv('full_data_test.csv')
View(test)
print(test[1,2])
print(test[1,2])
corrplot(test, method="color")
plot(test)
plot(test)
test <- read.csv('full_data_test.csv')
View(test)
test <- read.csv('full_data_test.csv')
test$budget[test$budget == 0] <- NA
test$revenue[test$revenue == 0] <- NA
for(i in 1:ncol(test)){
test[is.na(test[,i]), i] <- median(test[,i], na.rm = TRUE)
}
test <- test[, -1]
write.csv(test, 'clean_test.csv')
eps.model <- eps(train.mldr, "RF", cores=2)
View(test)
View(test)
val <- read.csv('clean_val.csv')
setwd("~/Repos/cs109b-final-project/milestone5")
val <- read.csv('clean_val.csv')
View(val)
train <- read.csv('clean_val.csv')
View(train)
id.col <- first.y <- which(colnames(train) == "id")
first.y <- which(colnames(train) == "Action")
last.y <- which(colnames(train) == "Western")
train <- train[, -id]
id.col <- first.y <- which(colnames(train) == "id")
train <- train[, -id.col]
View(train)
