{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification By Movie Overview\n",
    "Classification of movie genre using movie overviews from TMDB exclusively, using the text featurization methods of bigrams and Doc2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from gensim.models import doc2vec\n",
    "from gensim.parsing.preprocessing import stem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Movies for which we have posters\n",
    "We want to use the same movies for train, validation and test across all our classification tasks, so let's make sure the movies that we're using for text classification are the same ones we're using for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "movie_names = []\n",
    "for subdirectory in ['train', 'test', 'validation']:\n",
    "    movie_names += listdir(\"data/posters_split/\" + subdirectory)\n",
    "movie_ids = [filename.split(\"-\")[-1].split(\"_\")[0] for filename in movie_names]\n",
    "movie_ids = [int(id) for id in movie_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "We need to preprocess the movie overviews by stripping out punctuation and turning everything to lowercase. We deliberately choose to avoid stop word removal and stemming (both common NLP procedures) due to the relatively short length of the movie overviews. \n",
    "\n",
    "We also employ a 70/30 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31173, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>War</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Music</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>History</th>\n",
       "      <th>Western</th>\n",
       "      <th>...</th>\n",
       "      <th>TV_Movie</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Science_Fiction</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Action</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329865</td>\n",
       "      <td>Arrival</td>\n",
       "      <td>Taking place after alien crafts land around th...</td>\n",
       "      <td>[53, 18, 878, 9648]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210577</td>\n",
       "      <td>Gone Girl</td>\n",
       "      <td>With his wife's disappearance having become th...</td>\n",
       "      <td>[9648, 53, 18]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27205</td>\n",
       "      <td>Inception</td>\n",
       "      <td>Cobb, a skilled thief who commits corporate es...</td>\n",
       "      <td>[28, 53, 878, 9648, 12]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158852</td>\n",
       "      <td>Tomorrowland</td>\n",
       "      <td>Bound by a shared destiny, a bright, optimisti...</td>\n",
       "      <td>[28, 10751, 878, 12, 9648]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198663</td>\n",
       "      <td>The Maze Runner</td>\n",
       "      <td>Set in a post-apocalyptic world, young Thomas ...</td>\n",
       "      <td>[28, 9648, 878, 53]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_id      movie_title  \\\n",
       "0   329865          Arrival   \n",
       "1   210577        Gone Girl   \n",
       "2    27205        Inception   \n",
       "3   158852     Tomorrowland   \n",
       "4   198663  The Maze Runner   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Taking place after alien crafts land around th...   \n",
       "1  With his wife's disappearance having become th...   \n",
       "2  Cobb, a skilled thief who commits corporate es...   \n",
       "3  Bound by a shared destiny, a bright, optimisti...   \n",
       "4  Set in a post-apocalyptic world, young Thomas ...   \n",
       "\n",
       "                     genre_id  War  Crime  Music  Comedy  History  Western  \\\n",
       "0         [53, 18, 878, 9648]  0.0    0.0    0.0     0.0      0.0      0.0   \n",
       "1              [9648, 53, 18]  0.0    0.0    0.0     0.0      0.0      0.0   \n",
       "2     [28, 53, 878, 9648, 12]  0.0    0.0    0.0     0.0      0.0      0.0   \n",
       "3  [28, 10751, 878, 12, 9648]  0.0    0.0    0.0     0.0      0.0      0.0   \n",
       "4         [28, 9648, 878, 53]  0.0    0.0    0.0     0.0      0.0      0.0   \n",
       "\n",
       "    ...    TV_Movie  Fantasy  Animation  Drama  Documentary  Science_Fiction  \\\n",
       "0   ...         0.0      0.0        0.0    1.0          0.0              1.0   \n",
       "1   ...         0.0      0.0        0.0    1.0          0.0              0.0   \n",
       "2   ...         0.0      0.0        0.0    0.0          0.0              1.0   \n",
       "3   ...         0.0      0.0        0.0    0.0          0.0              1.0   \n",
       "4   ...         0.0      0.0        0.0    0.0          0.0              1.0   \n",
       "\n",
       "   Horror  Action  Romance  Family  \n",
       "0     0.0     0.0      0.0     0.0  \n",
       "1     0.0     0.0      0.0     0.0  \n",
       "2     0.0     1.0      0.0     0.0  \n",
       "3     0.0     1.0      0.0     1.0  \n",
       "4     0.0     1.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas has issues reading in a dataframe this large\n",
    "# so read in chunk by chunk instead\n",
    "full_dataset = pd.concat(pd.read_csv('data/full_raw_movie_overviews.csv', iterator=True, chunksize=1000), ignore_index=True)\n",
    "print full_dataset.shape\n",
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20291, 23)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where overview is missing\n",
    "full_dataset = full_dataset.dropna()\n",
    "print full_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10509, 23)\n"
     ]
    }
   ],
   "source": [
    "# Filter for movies that we have posters for, dropping duplicates\n",
    "full_dataset = full_dataset[full_dataset['movie_id'].isin(movie_ids)].drop_duplicates()\n",
    "print full_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to lowercase and remove punctuation\n",
    "full_dataset['overview'] = full_dataset['overview'].str.lower()\n",
    "full_dataset['overview'] = full_dataset['overview'].apply(lambda x: x.translate(string.maketrans(\" \",\" \"), string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8405, 23) (1049, 23) (1055, 23)\n"
     ]
    }
   ],
   "source": [
    "# Use same test-train-validation split as posters\n",
    "train_val_test_byid = pickle.load(open('data/train_val_test_byid.pkl', 'rb'))\n",
    "train = full_dataset[full_dataset['movie_id'].isin(train_val_test_byid['train'])]\n",
    "test = full_dataset[full_dataset['movie_id'].isin(train_val_test_byid['test'])]\n",
    "validation = full_dataset[full_dataset['movie_id'].isin(train_val_test_byid['validation'])]\n",
    "print train.shape, test.shape, validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "count_vectorizer.fit(train[\"overview\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate bigram features\n",
    "train_bigrams = count_vectorizer.transform(train[\"overview\"])\n",
    "val_bigrams = count_vectorizer.transform(validation[\"overview\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec feature generation\n",
    "We use doc2vec to learn fixed length vector representations of each of the documents (movie overviews) and therefore avoid time intensive route of manual feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedXML = namedtuple('TaggedXML', 'words tags')\n",
    "documents = []\n",
    "\n",
    "for i in train.index:\n",
    "    tags = [train['movie_id'][i]]\n",
    "    documents.append(taggedXML(train['overview'][i].split(), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2v = doc2vec.Doc2Vec(documents, size=100, workers=4, iter=30)\n",
    "# Save model\n",
    "d2v.save(\"d2v_models/d2v_size100_iter30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use trained d2v model in order to infer feature vectors for each of the documents\n",
    "train_d2v_features = [d2v.infer_vector(document) for document in train[\"overview\"]]\n",
    "val_d2v_features = [d2v.infer_vector(document) for document in validation[\"overview\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multilabel Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get labels into multilabel classification format for train and test\n",
    "train_y = train.ix[:, 'War':]\n",
    "val_y = validation.ix[:, 'War':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023696682464454975"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train random forest on bigram features\n",
    "bigram_rf = RandomForestClassifier(n_estimators=10)\n",
    "bigram_rf.fit(train_bigrams, train_y)\n",
    "\n",
    "train_bigram_pred = bigram_rf.predict(train_bigrams)\n",
    "val_bigram_pred = bigram_rf.predict(val_bigrams)\n",
    "\n",
    "bigram_rf.score(val_bigrams, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss on train with bigram features: 0.0176023043927\n",
      "Hamming loss on validation with bigram features: 0.134696931903\n"
     ]
    }
   ],
   "source": [
    "print \"Hamming loss on train with bigram features:\", hamming_loss(train_y, train_bigram_pred)\n",
    "print \"Hamming loss on validation with bigram features:\", hamming_loss(val_y, val_bigram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0056872037914691941"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest on doc2vec features\n",
    "d2v_rf = RandomForestClassifier(n_estimators=10)\n",
    "d2v_rf.fit(train_d2v_features, train_y)\n",
    "\n",
    "train_d2v_pred = d2v_rf.predict(train_d2v_features)\n",
    "val_d2v_pred = d2v_rf.predict(val_d2v_features)\n",
    "\n",
    "d2v_rf.score(val_d2v_features, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss on train with d2v features: 0.0214095619775\n",
      "Hamming loss on validation with d2v features: 0.141032676478\n"
     ]
    }
   ],
   "source": [
    "print \"Hamming loss on train with d2v features:\",  hamming_loss(train_y, train_d2v_pred)\n",
    "print \"Hamming loss on validation with d2v features:\", hamming_loss(val_y, val_d2v_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive 0 Classifier (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss on train with naive all 0 classifier: 0.14239644322\n",
      "Hamming loss on validation with naive all 0 classifier: 0.139835370417\n"
     ]
    }
   ],
   "source": [
    "print \"Hamming loss on train with naive all 0 classifier:\",  hamming_loss(train_y, np.zeros((train_d2v_pred.shape)))\n",
    "print \"Hamming loss on validation with naive all 0 classifier:\",  hamming_loss(val_y, np.zeros((val_d2v_pred.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes on Bigram Features, One v. Rest Classifier for Multilabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016113744075829384"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest on bigram features\n",
    "nb = OneVsRestClassifier(MultinomialNB())\n",
    "nb.fit(train_bigrams, train_y)\n",
    "\n",
    "train_nb_pred = nb.predict(train_bigrams)\n",
    "val_nb_pred = nb.predict(val_bigrams)\n",
    "\n",
    "nb.score(val_bigrams, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss on train with naive bayes: 0.020470271455\n",
      "Hamming loss on validation with naive bayes: 0.128261411823\n"
     ]
    }
   ],
   "source": [
    "print \"Hamming loss on train with naive bayes:\",  hamming_loss(train_y, train_nb_pred)\n",
    "print \"Hamming loss on validation with naive bayes:\", hamming_loss(val_y, val_nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>War</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Music</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>History</th>\n",
       "      <th>Western</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>TV_Movie</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Science_Fiction</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Action</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7711</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7917</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8054</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8306</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26891</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27489</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27574</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27613</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27728</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27741</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27831</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27885</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27895</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30554</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       War  Crime  Music  Comedy  History  Western  Thriller  Mystery  \\\n",
       "6726   0.0    1.0    0.0     0.0      0.0      0.0       1.0      0.0   \n",
       "7711   0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "7917   0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "8054   0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "8219   0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "8306   0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "26891  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27489  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27574  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27613  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27645  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27728  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27741  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27831  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27885  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "27895  0.0    0.0    0.0     0.0      0.0      0.0       0.0      0.0   \n",
       "30554  0.0    0.0    0.0     0.0      1.0      0.0       0.0      0.0   \n",
       "\n",
       "       Adventure  TV_Movie  Fantasy  Animation  Drama  Documentary  \\\n",
       "6726         0.0       0.0      0.0        0.0    0.0          0.0   \n",
       "7711         0.0       0.0      0.0        0.0    1.0          0.0   \n",
       "7917         0.0       0.0      0.0        0.0    1.0          0.0   \n",
       "8054         0.0       0.0      0.0        0.0    1.0          0.0   \n",
       "8219         0.0       0.0      0.0        0.0    1.0          0.0   \n",
       "8306         0.0       0.0      0.0        0.0    1.0          0.0   \n",
       "26891        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27489        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27574        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27613        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27645        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27728        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27741        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27831        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27885        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "27895        0.0       0.0      0.0        0.0    0.0          1.0   \n",
       "30554        0.0       0.0      0.0        0.0    1.0          0.0   \n",
       "\n",
       "       Science_Fiction  Horror  Action  Romance  Family  \n",
       "6726               0.0     0.0     1.0      0.0     0.0  \n",
       "7711               0.0     0.0     0.0      0.0     0.0  \n",
       "7917               0.0     0.0     0.0      0.0     0.0  \n",
       "8054               0.0     0.0     0.0      0.0     0.0  \n",
       "8219               0.0     0.0     0.0      0.0     0.0  \n",
       "8306               0.0     0.0     0.0      0.0     0.0  \n",
       "26891              0.0     0.0     0.0      0.0     0.0  \n",
       "27489              0.0     0.0     0.0      0.0     0.0  \n",
       "27574              0.0     0.0     0.0      0.0     0.0  \n",
       "27613              0.0     0.0     0.0      0.0     0.0  \n",
       "27645              0.0     0.0     0.0      0.0     0.0  \n",
       "27728              0.0     0.0     0.0      0.0     0.0  \n",
       "27741              0.0     0.0     0.0      0.0     0.0  \n",
       "27831              0.0     0.0     0.0      0.0     0.0  \n",
       "27885              0.0     0.0     0.0      0.0     0.0  \n",
       "27895              0.0     0.0     0.0      0.0     0.0  \n",
       "30554              0.0     0.0     0.0      0.0     0.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the datapoints for which the naive bayes predictions match the actual \n",
    "# multilabels in the validation set\n",
    "val_y[(val_y == val_nb_pred).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately from the abysmal performance in terms of both Hamming loss and subset accuracy, it does not look like predicting by movie overview alone is a feasible approach for this multilabel classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
