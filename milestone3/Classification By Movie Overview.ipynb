{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification By Movie Overview\n",
    "Classification of movie genre using movie overviews from TMDB exclusively, using the text featurization methods of bigrams and Doc2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from gensim.models import doc2vec\n",
    "from gensim.parsing.preprocessing import stem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import hamming_loss\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Movies for which we have posters\n",
    "We want to use the same movies to train and test across our classification tasks, so let's make sure the movies that we're using for text classification are the same ones we're using for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "movie_names = []\n",
    "with open(\"data/genre_dict.pickle\", 'rb') as handle:\n",
    "    genres = pickle.load(handle)\n",
    "for genre_name in genres.values():\n",
    "    movie_names += listdir(\"data/posters/\" + genre_name)\n",
    "movie_ids = [filename.split(\"-\")[-1].split(\"_\")[0] for filename in movie_names]\n",
    "movie_ids = [int(id) for id in movie_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "We need to preprocess the movie overviews by stripping out punctuation and turning everything to lowercase. We deliberately choose to avoid stop word removal and stemming (both common NLP procedures) due to the relatively short length of the movie overviews. \n",
    "\n",
    "We also employ an 70/30 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2260, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>War</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Music</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>History</th>\n",
       "      <th>Western</th>\n",
       "      <th>...</th>\n",
       "      <th>TV_Movie</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Science_Fiction</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Action</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329865</td>\n",
       "      <td>Arrival</td>\n",
       "      <td>Taking place after alien crafts land around th...</td>\n",
       "      <td>[53, 18, 878, 9648]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210577</td>\n",
       "      <td>Gone Girl</td>\n",
       "      <td>With his wife's disappearance having become th...</td>\n",
       "      <td>[9648, 53, 18]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419430</td>\n",
       "      <td>Get Out</td>\n",
       "      <td>A young black man visits his white girlfriend'...</td>\n",
       "      <td>[27, 9648, 53]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27205</td>\n",
       "      <td>Inception</td>\n",
       "      <td>Cobb, a skilled thief who commits corporate es...</td>\n",
       "      <td>[28, 53, 878, 9648, 12]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>273248</td>\n",
       "      <td>The Hateful Eight</td>\n",
       "      <td>Bounty hunters seek shelter from a raging bliz...</td>\n",
       "      <td>[80, 18, 9648, 37]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id        movie_title  \\\n",
       "0    329865            Arrival   \n",
       "1    210577          Gone Girl   \n",
       "2    419430            Get Out   \n",
       "3     27205          Inception   \n",
       "4    273248  The Hateful Eight   \n",
       "\n",
       "                                            overview                 genre_id  \\\n",
       "0  Taking place after alien crafts land around th...      [53, 18, 878, 9648]   \n",
       "1  With his wife's disappearance having become th...           [9648, 53, 18]   \n",
       "2  A young black man visits his white girlfriend'...           [27, 9648, 53]   \n",
       "3  Cobb, a skilled thief who commits corporate es...  [28, 53, 878, 9648, 12]   \n",
       "4  Bounty hunters seek shelter from a raging bliz...       [80, 18, 9648, 37]   \n",
       "\n",
       "   War  Crime  Music  Comedy  History  Western   ...    TV_Movie  Fantasy  \\\n",
       "0    0      0      0       0        0        0   ...           0        0   \n",
       "1    0      0      0       0        0        0   ...           0        0   \n",
       "2    0      0      0       0        0        0   ...           0        0   \n",
       "3    0      0      0       0        0        0   ...           0        0   \n",
       "4    0      1      0       0        0        1   ...           0        0   \n",
       "\n",
       "   Animation  Drama  Documentary  Science_Fiction  Horror  Action  Romance  \\\n",
       "0          0      1            0                1       0       0        0   \n",
       "1          0      1            0                0       0       0        0   \n",
       "2          0      0            0                0       1       0        0   \n",
       "3          0      0            0                1       0       1        0   \n",
       "4          0      1            0                0       0       0        0   \n",
       "\n",
       "   Family  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = pd.read_csv('data/full_raw_movie_overviews.csv')\n",
    "print full_dataset.shape\n",
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2248, 23)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where overview is missing\n",
    "full_dataset = full_dataset.dropna()\n",
    "print full_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1177, 23)\n"
     ]
    }
   ],
   "source": [
    "# Filter for movies that we have posters for, dropping duplicates\n",
    "full_dataset = full_dataset[full_dataset['movie_id'].isin(movie_ids)].drop_duplicates()\n",
    "print full_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to lowercase and remove punctuation\n",
    "full_dataset['overview'] = full_dataset['overview'].str.lower()\n",
    "full_dataset['overview'] = full_dataset['overview'].apply(lambda x: x.translate(string.maketrans(\" \",\" \"), string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly Train-test split, 70-30\n",
    "train, test = train_test_split(full_dataset, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "train.to_csv(\"data/preprocessed_overviews_train_small.csv\", index=False)\n",
    "test.to_csv(\"data/preprocessed_overviews_test_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "count_vectorizer.fit(train[\"overview\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate bigram features\n",
    "train_bigrams = count_vectorizer.transform(train[\"overview\"])\n",
    "test_bigrams = count_vectorizer.transform(test[\"overview\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec feature generation\n",
    "We use doc2vec to learn fixed length vector representations of each of the documents (movie overviews) and therefore avoid time intensive route of manual feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedXML = namedtuple('TaggedXML', 'words tags')\n",
    "documents = []\n",
    "\n",
    "for i in train.index:\n",
    "    tags = [train['movie_id'][i]]\n",
    "    documents.append(taggedXML(train['overview'][i].split(), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2v = doc2vec.Doc2Vec(documents, size=10, workers=4, iter=30)\n",
    "# Save model\n",
    "d2v.save(\"d2v_models/d2v_size10_iter30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use trained d2v model in order to infer feature vectors for each of the documents\n",
    "train_d2v_features = [d2v.infer_vector(document) for document in train[\"overview\"]]\n",
    "test_d2v_features = [d2v.infer_vector(document) for document in test[\"overview\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multilabel Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get labels into multilabel classification format for train and test\n",
    "train_y = train.ix[:, 'War':]\n",
    "test_y = test.ix[:, 'War':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028248587570621469"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest on bigram features\n",
    "bigram_rf = RandomForestClassifier(n_estimators=10)\n",
    "bigram_rf.fit(train_bigrams, train_y)\n",
    "\n",
    "train_bigram_pred = bigram_rf.predict(train_bigrams)\n",
    "test_bigram_pred = bigram_rf.predict(test_bigrams)\n",
    "\n",
    "bigram_rf.score(test_bigrams, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss on train with bigram features: 0.0227025644305\n",
      "Hamming loss on test with bigram features: 0.150609574784\n"
     ]
    }
   ],
   "source": [
    "print \"Hamming loss on train with bigram features:\", hamming_loss(train_y, train_bigram_pred)\n",
    "print \"Hamming loss on test with bigram features:\", hamming_loss(test_y, test_bigram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest on doc2vec features\n",
    "d2v_rf = RandomForestClassifier(n_estimators=10)\n",
    "d2v_rf.fit(train_d2v_features, train_y)\n",
    "\n",
    "train_d2v_pred = d2v_rf.predict(train_d2v_features)\n",
    "test_d2v_pred = d2v_rf.predict(test_d2v_features)\n",
    "\n",
    "d2v_rf.score(test_d2v_features, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss on train with d2v features: 0.0248129436593\n",
      "Hamming loss on test with d2v features: 0.16696402022\n"
     ]
    }
   ],
   "source": [
    "print \"Hamming loss on train with d2v features:\",  hamming_loss(train_y, train_d2v_pred)\n",
    "print \"Hamming loss on test with d2v features:\", hamming_loss(test_y, test_d2v_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hamming loss on train with naive all 0 classifier: 0.156487817356\n",
      "Hamming loss on test with naive all 0 classifier: 0.155961938745\n"
     ]
    }
   ],
   "source": [
    "print \"Hamming loss on train with naive all 0 classifier:\",  hamming_loss(train_y, np.zeros((train_d2v_pred.shape)))\n",
    "print \"Hamming loss on test with naive all 0 classifier:\",  hamming_loss(test_y, np.zeros((test_d2v_pred.shape)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
